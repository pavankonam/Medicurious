{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Drug Name            Active Ingredient  \\\n",
      "0  ABACAVIR and LAMIVUDINE  ABACAVIR SULFATE;LAMIVUDINE   \n",
      "1                  ABILIFY                 ARIPIPRAZOLE   \n",
      "2                  ABILIFY                 ARIPIPRAZOLE   \n",
      "3                  ABILIFY                 ARIPIPRAZOLE   \n",
      "4                  ABILIFY                 ARIPIPRAZOLE   \n",
      "\n",
      "                          Form; Route  Appl. No.                     Company  \\\n",
      "0                         TABLET;ORAL     204311  MYLAN LABORATORIES LIMITED   \n",
      "1                       SOLUTION;ORAL      21713                      OTSUKA   \n",
      "2  TABLET, ORALLY DISINTEGRATING;ORAL      21729                      OTSUKA   \n",
      "3                         TABLET;ORAL      21436                      OTSUKA   \n",
      "4            INJECTABLE;INTRAMUSCULAR      21866                      OTSUKA   \n",
      "\n",
      "         Date                                                URL  \n",
      "0  12/22/2023  https://www.accessdata.fda.gov/drugsatfda_docs...  \n",
      "1  02/05/2020  https://www.accessdata.fda.gov/drugsatfda_docs...  \n",
      "2  02/05/2020  https://www.accessdata.fda.gov/drugsatfda_docs...  \n",
      "3  11/30/2022  https://www.accessdata.fda.gov/drugsatfda_docs...  \n",
      "4  02/05/2020  https://www.accessdata.fda.gov/drugsatfda_docs...  \n",
      "Index(['Drug Name', 'Active Ingredient', 'Form; Route', 'Appl. No.', 'Company',\n",
      "       'Date', 'URL'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/60/m4vjsxs154b8jqhld_x5c2j80000gn/T/ipykernel_35905/4076151070.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  medication_data[\"Drug Name\"] = medication_data[\"Drug Name\"].apply(clean_drug_name)\n",
      "/var/folders/60/m4vjsxs154b8jqhld_x5c2j80000gn/T/ipykernel_35905/4076151070.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  medication_data[\"Active Ingredient\"] = medication_data[\"Active Ingredient\"].apply(clean_drug_name)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load medication guides dataset\n",
    "medication_guides = pd.read_csv(\"csv/Medication Guides.csv\")\n",
    "\n",
    "# Display basic info\n",
    "print(medication_guides.head())\n",
    "print(medication_guides.columns)\n",
    "# Extract relevant columns\n",
    "relevant_columns = [\"Drug Name\", \"Active Ingredient\", \"Form; Route\", \"Date\", \"URL\"]\n",
    "medication_data = medication_guides[relevant_columns]\n",
    "\n",
    "# Clean and preprocess data\n",
    "def clean_drug_name(name):\n",
    "    return name.strip().lower() if isinstance(name, str) else \"\"\n",
    "\n",
    "medication_data[\"Drug Name\"] = medication_data[\"Drug Name\"].apply(clean_drug_name)\n",
    "medication_data[\"Active Ingredient\"] = medication_data[\"Active Ingredient\"].apply(clean_drug_name)\n",
    "\n",
    "# Save cleaned data\n",
    "medication_data.to_csv(\"csv/cleaned_medication_guides.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   itching  skin_rash  nodal_skin_eruptions  continuous_sneezing  shivering  \\\n",
      "0        1          1                     1                    0          0   \n",
      "1        0          1                     1                    0          0   \n",
      "2        1          0                     1                    0          0   \n",
      "3        1          1                     0                    0          0   \n",
      "4        1          1                     1                    0          0   \n",
      "\n",
      "   chills  joint_pain  stomach_pain  acidity  ulcers_on_tongue  ...  \\\n",
      "0       0           0             0        0                 0  ...   \n",
      "1       0           0             0        0                 0  ...   \n",
      "2       0           0             0        0                 0  ...   \n",
      "3       0           0             0        0                 0  ...   \n",
      "4       0           0             0        0                 0  ...   \n",
      "\n",
      "   blackheads  scurring  skin_peeling  silver_like_dusting  \\\n",
      "0           0         0             0                    0   \n",
      "1           0         0             0                    0   \n",
      "2           0         0             0                    0   \n",
      "3           0         0             0                    0   \n",
      "4           0         0             0                    0   \n",
      "\n",
      "   small_dents_in_nails  inflammatory_nails  blister  red_sore_around_nose  \\\n",
      "0                     0                   0        0                     0   \n",
      "1                     0                   0        0                     0   \n",
      "2                     0                   0        0                     0   \n",
      "3                     0                   0        0                     0   \n",
      "4                     0                   0        0                     0   \n",
      "\n",
      "   yellow_crust_ooze         prognosis  \n",
      "0                  0  Fungal Infection  \n",
      "1                  0  Fungal Infection  \n",
      "2                  0  Fungal Infection  \n",
      "3                  0  Fungal Infection  \n",
      "4                  0  Fungal Infection  \n",
      "\n",
      "[5 rows x 133 columns]\n",
      "itching                 0\n",
      "skin_rash               0\n",
      "nodal_skin_eruptions    0\n",
      "continuous_sneezing     0\n",
      "shivering               0\n",
      "                       ..\n",
      "inflammatory_nails      0\n",
      "blister                 0\n",
      "red_sore_around_nose    0\n",
      "yellow_crust_ooze       0\n",
      "prognosis               0\n",
      "Length: 133, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load symptom prediction dataset\n",
    "symptom_data = pd.read_csv(\"csv/symbipredict_2022.csv\")\n",
    "\n",
    "# Display basic info\n",
    "print(symptom_data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(symptom_data.isnull().sum())\n",
    "\n",
    "# Separate symptoms and prognosis\n",
    "symptoms = symptom_data.iloc[:, :-1]  # All columns except the last one\n",
    "prognosis = symptom_data[\"prognosis\"]\n",
    "\n",
    "# Convert binary symptoms to a list of active symptoms for each row\n",
    "def get_active_symptoms(row):\n",
    "    return [col for col, value in row.items() if value == 1]\n",
    "\n",
    "symptom_data[\"active_symptoms\"] = symptoms.apply(get_active_symptoms, axis=1)\n",
    "\n",
    "# Combine active symptoms and prognosis into a single dataset\n",
    "processed_data = pd.DataFrame({\n",
    "    \"symptoms\": symptom_data[\"active_symptoms\"],\n",
    "    \"diagnosis\": prognosis\n",
    "})\n",
    "\n",
    "# Save processed data\n",
    "processed_data.to_json(\"csv/processed_symptom_data.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load processed symptom data\n",
    "with open(\"csv/processed_symptom_data.jsonl\", \"r\") as f:\n",
    "    symptom_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Load medication data\n",
    "medication_data = pd.read_csv(\"csv/cleaned_medication_guides.csv\")\n",
    "\n",
    "# Create combined dataset\n",
    "combined_data = []\n",
    "\n",
    "# Add symptom-diagnosis data\n",
    "for entry in symptom_data:\n",
    "    combined_data.append({\n",
    "        \"input\": \", \".join(entry[\"symptoms\"]),\n",
    "        \"output\": f\"Diagnosis: {entry['diagnosis']}\"\n",
    "    })\n",
    "\n",
    "# Add medication data\n",
    "for _, row in medication_data.iterrows():\n",
    "    combined_data.append({\n",
    "        \"input\": f\"Drug Name: {row['Drug Name']}, Active Ingredient: {row['Active Ingredient']}\",\n",
    "        \"output\": f\"Form: {row['Form; Route']}, Last Updated: {row['Date']}, More Info: {row['URL']}\"\n",
    "    })\n",
    "\n",
    "# Save combined data as JSONL\n",
    "with open(\"combined_data.jsonl\", \"w\") as f:\n",
    "    for item in combined_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guideline text extracted and saved.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # Extract text\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:  # Ensure text exists\n",
    "                text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Extract text from guideline-170-en.pdf\n",
    "guideline_text = extract_text_from_pdf(\"pdf/guideline-170-en.pdf\")\n",
    "\n",
    "# Save extracted text to a file\n",
    "with open(\"pdf/guideline_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(guideline_text)\n",
    "\n",
    "print(\"Guideline text extracted and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guideline tables extracted and saved.\n"
     ]
    }
   ],
   "source": [
    "# Function to extract tables from PDF\n",
    "def extract_tables_from_pdf(pdf_path):\n",
    "    tables = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # Extract tables\n",
    "            page_tables = page.extract_tables()\n",
    "            if page_tables:\n",
    "                tables.extend(page_tables)\n",
    "    return tables\n",
    "\n",
    "# Extract tables from guideline-170-en.pdf\n",
    "guideline_tables = extract_tables_from_pdf(\"pdf/guideline-170-en.pdf\")\n",
    "\n",
    "# Save tables to a file\n",
    "with open(\"pdf/guideline_tables.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for table in guideline_tables:\n",
    "        f.write(str(table) + \"\\n\\n\")\n",
    "\n",
    "print(\"Guideline tables extracted and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug safety data and conclusion definitions extracted and saved.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "\n",
    "# Function to extract drug safety data\n",
    "def extract_drug_safety_data(pdf_path):\n",
    "    data = []\n",
    "    conclusion_definitions = {}\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "\n",
    "            # Extract table rows using regex\n",
    "            rows = re.findall(r\"([A-Za-z\\s]+)\\s+([A-Za-z\\s]+)\\s+([A-Za-z\\s\\-]+)\\s+([A-Za-z\\s\\-]+)\\s+([A-Za-z\\!\\?]+)\\s+([\\d\\,\\s]+)\", text)\n",
    "            for row in rows:\n",
    "                class_, type_, generic_name, brand_name, conclusion, references = row\n",
    "                data.append({\n",
    "                    \"Class\": class_.strip(),\n",
    "                    \"Type\": type_.strip(),\n",
    "                    \"Generic Name\": generic_name.strip(),\n",
    "                    \"Brand Name\": brand_name.strip(),\n",
    "                    \"Conclusion\": conclusion.strip(),\n",
    "                    \"References\": references.strip()\n",
    "                })\n",
    "\n",
    "            # Extract conclusion definitions\n",
    "            if \"Definitions\" in text:\n",
    "                definitions = re.findall(r\"([A-Za-z\\!\\?]+):\\s+(.+)\", text)\n",
    "                for key, value in definitions:\n",
    "                    conclusion_definitions[key.strip()] = value.strip()\n",
    "\n",
    "    return data, conclusion_definitions\n",
    "\n",
    "# Extract drug safety data and definitions\n",
    "drug_safety_data, conclusion_definitions = extract_drug_safety_data(\"pdf/Drug Safety Database Print - American Porphyria Foundation.pdf\")\n",
    "\n",
    "# Save drug safety data to JSON\n",
    "import json\n",
    "with open(\"pdf/drug_safety_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(drug_safety_data, f, indent=4)\n",
    "\n",
    "# Save conclusion definitions to JSON\n",
    "with open(\"pdf/conclusion_definitions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(conclusion_definitions, f, indent=4)\n",
    "\n",
    "print(\"Drug safety data and conclusion definitions extracted and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug safety data normalized and saved.\n"
     ]
    }
   ],
   "source": [
    "# Normalize drug safety data\n",
    "for entry in drug_safety_data:\n",
    "    for key, value in entry.items():\n",
    "        entry[key] = value.strip().lower() if isinstance(value, str) else value\n",
    "\n",
    "# Save normalized data\n",
    "with open(\"pdf/normalized_drug_safety_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(drug_safety_data, f, indent=4)\n",
    "\n",
    "print(\"Drug safety data normalized and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved as JSONL.\n"
     ]
    }
   ],
   "source": [
    "# Load guideline text\n",
    "with open(\"pdf/guideline_text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    guideline_text = f.read()\n",
    "\n",
    "# Load drug safety data\n",
    "with open(\"pdf/normalized_drug_safety_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    drug_safety_data = json.load(f)\n",
    "\n",
    "# Combine data into a single JSONL file\n",
    "combined_data = []\n",
    "\n",
    "# Add guideline text\n",
    "combined_data.append({\"input\": \"Clinical Guidelines\", \"output\": guideline_text})\n",
    "\n",
    "# Add drug safety data\n",
    "for entry in drug_safety_data:\n",
    "    combined_data.append({\n",
    "        \"input\": f\"{entry['Generic Name']} ({entry['Brand Name']})\",\n",
    "        \"output\": f\"Class: {entry['Class']}, Type: {entry['Type']}, Conclusion: {entry['Conclusion']}, References: {entry['References']}\"\n",
    "    })\n",
    "\n",
    "# Save combined data\n",
    "with open(\"combined_data_pdf.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in combined_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(\"Combined data saved as JSONL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 42 entries from combined_data_pdf.jsonl\n",
      "Added 5923 entries from combined_data.jsonl\n",
      "Combined data saved to combined_data_final.jsonl with 5965 total entries\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# File paths for the two input files and the output file\n",
    "file1_path = \"combined_data_pdf.jsonl\"  # Adjust this to your first file's name/path\n",
    "file2_path = \"combined_data.jsonl\"  # Adjust this to your second file's name/path\n",
    "output_path = \"combined_data_final.jsonl\"\n",
    "\n",
    "# Step 1: Read and combine the data from both files\n",
    "combined_data = []\n",
    "\n",
    "# Read the first file\n",
    "try:\n",
    "    with open(file1_path, \"r\") as f1:\n",
    "        for line in f1:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                combined_data.append(json.loads(line))\n",
    "    print(f\"Loaded {len(combined_data)} entries from {file1_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {file1_path} not found\")\n",
    "    exit()\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON in {file1_path}: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Read the second file\n",
    "try:\n",
    "    with open(file2_path, \"r\") as f2:\n",
    "        initial_count = len(combined_data)\n",
    "        for line in f2:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                combined_data.append(json.loads(line))\n",
    "    print(f\"Added {len(combined_data) - initial_count} entries from {file2_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {file2_path} not found\")\n",
    "    exit()\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON in {file2_path}: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Write the combined data to a new file\n",
    "try:\n",
    "    with open(output_path, \"w\") as f_out:\n",
    "        for item in combined_data:\n",
    "            f_out.write(json.dumps(item) + \"\\n\")\n",
    "    print(f\"Combined data saved to {output_path} with {len(combined_data)} total entries\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to {output_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved as Simple_Knowledge_Graph.html\n",
      "Graph saved as Complex_Knowledge_Graph.html\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "    return data\n",
    "\n",
    "def build_graph(data, simple=False):\n",
    "    G = nx.DiGraph()\n",
    "    sample_size = 5 if simple else len(data)\n",
    "    \n",
    "    for entry in data[:sample_size]:\n",
    "        user_query = entry['messages'][1]['content']\n",
    "        assistant_response = entry['messages'][2]['content']\n",
    "        \n",
    "        # Extract structured attributes correctly\n",
    "        response_parts = assistant_response.split(', ')\n",
    "        attributes = {}\n",
    "        for part in response_parts:\n",
    "            if ': ' in part:\n",
    "                key, value = part.split(': ')\n",
    "                attributes[key.strip()] = value.strip()\n",
    "            else:\n",
    "                attributes.setdefault('Other', []).append(part.strip())\n",
    "        \n",
    "        G.add_node(user_query, color='blue', size=20)\n",
    "        for key, value in attributes.items():\n",
    "            if isinstance(value, list):\n",
    "                for v in value:\n",
    "                    G.add_node(v, color='red', size=10)\n",
    "                    G.add_edge(user_query, v, label=key)\n",
    "            else:\n",
    "                G.add_node(value, color='red', size=10)\n",
    "                G.add_edge(user_query, value, label=key)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def visualize_graph(G, title=\"Knowledge Graph\"):\n",
    "    net = Network(notebook=False, height=\"800px\", width=\"100%\", directed=True)\n",
    "    \n",
    "    for node, data in G.nodes(data=True):\n",
    "        net.add_node(node, label=node, color=data.get('color', 'black'), size=data.get('size', 10))\n",
    "    \n",
    "    for source, target, data in G.edges(data=True):\n",
    "        net.add_edge(source, target, title=data.get('label', ''))\n",
    "    \n",
    "    html_file = title.replace(\" \", \"_\") + \".html\"\n",
    "    net.write_html(html_file)\n",
    "    print(f\"Graph saved as {html_file}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"/Users/pavankonam/Desktop/RRL_Project/transformed_data.jsonl\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Generate simple graph\n",
    "simple_G = build_graph(data, simple=True)\n",
    "visualize_graph(simple_G, \"Simple_Knowledge_Graph\")\n",
    "\n",
    "# Generate complex graph\n",
    "complex_G = build_graph(data, simple=False)\n",
    "visualize_graph(complex_G, \"Complex_Knowledge_Graph\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
